{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d048c6",
   "metadata": {},
   "source": [
    "#### Joongyeon Steven Cho\n",
    "#### Text Categorization using Keras with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569d9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb68145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = input(\"Name of labeled list of training file: \")\n",
    "\n",
    "train_list = open(train_file_name).read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48709fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\",\n",
    "             \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8efc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "train_articles = []\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-z #+_]')\n",
    "\n",
    "for line in train_list:\n",
    "    path = line.split(' ')[0]\n",
    "    category = line.split(' ')[1]\n",
    "    article = open(path).read()\n",
    "    \n",
    "    article = article.lower()\n",
    "    article = REPLACE_BY_SPACE_RE.sub(' ', article)\n",
    "\n",
    "    article = BAD_SYMBOLS_RE.sub('', article)\n",
    "    # print(article)\n",
    "\n",
    "    for word in stopwords:\n",
    "        token = \" \" + word + \" \"\n",
    "        article = article.replace(token, ' ')\n",
    "\n",
    "    categories.append(category)\n",
    "    train_articles.append(article)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdff32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numlen = 256\n",
    "dict_size = 5000\n",
    "train_ratio = 0.90\n",
    "embedding_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c164af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24010\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "train_size = int(len(train_articles)*train_ratio)\n",
    "training_articles = train_articles[0:train_size]\n",
    "train_categories = categories[0:train_size]\n",
    "\n",
    "tuning_articles = train_articles[train_size:]\n",
    "tuning_categories = categories[train_size:]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=dict_size, oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe585bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str': 1, 'pol': 2, 'dis': 3, 'cri': 4, 'oth': 5}\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(training_articles)\n",
    "train_padded = pad_sequences(\n",
    "    train_sequences, maxlen=numlen, padding=\"post\",truncating=\"post\")\n",
    "\n",
    "tuning_sequences = tokenizer.texts_to_sequences(tuning_articles)\n",
    "tuning_padded = pad_sequences(\n",
    "    tuning_sequences, maxlen=numlen,padding=\"post\", truncating=\"post\")  \n",
    "    \n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(categories)\n",
    "\n",
    "print(label_tokenizer.word_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73cf37ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 - 6s - loss: 1.7000 - accuracy: 0.3053 - val_loss: 1.4600 - val_accuracy: 0.3483 - 6s/epoch - 233ms/step\n",
      "Epoch 2/30\n",
      "25/25 - 3s - loss: 1.5590 - accuracy: 0.3103 - val_loss: 1.3637 - val_accuracy: 0.3483 - 3s/epoch - 118ms/step\n",
      "Epoch 3/30\n",
      "25/25 - 3s - loss: 1.3056 - accuracy: 0.4070 - val_loss: 1.0912 - val_accuracy: 0.5393 - 3s/epoch - 110ms/step\n",
      "Epoch 4/30\n",
      "25/25 - 3s - loss: 1.0835 - accuracy: 0.5126 - val_loss: 0.9339 - val_accuracy: 0.6292 - 3s/epoch - 114ms/step\n",
      "Epoch 5/30\n",
      "25/25 - 3s - loss: 0.9205 - accuracy: 0.5766 - val_loss: 0.7535 - val_accuracy: 0.7303 - 3s/epoch - 120ms/step\n",
      "Epoch 6/30\n",
      "25/25 - 3s - loss: 0.7280 - accuracy: 0.6947 - val_loss: 0.5733 - val_accuracy: 0.7865 - 3s/epoch - 114ms/step\n",
      "Epoch 7/30\n",
      "25/25 - 3s - loss: 0.4894 - accuracy: 0.8116 - val_loss: 0.4845 - val_accuracy: 0.8989 - 3s/epoch - 116ms/step\n",
      "Epoch 8/30\n",
      "25/25 - 3s - loss: 0.3675 - accuracy: 0.8907 - val_loss: 0.3938 - val_accuracy: 0.8876 - 3s/epoch - 122ms/step\n",
      "Epoch 9/30\n",
      "25/25 - 3s - loss: 0.2359 - accuracy: 0.9171 - val_loss: 0.3783 - val_accuracy: 0.8427 - 3s/epoch - 119ms/step\n",
      "Epoch 10/30\n",
      "25/25 - 3s - loss: 0.1987 - accuracy: 0.9460 - val_loss: 0.4392 - val_accuracy: 0.8764 - 3s/epoch - 117ms/step\n",
      "Epoch 11/30\n",
      "25/25 - 3s - loss: 0.1356 - accuracy: 0.9661 - val_loss: 0.4801 - val_accuracy: 0.8539 - 3s/epoch - 122ms/step\n",
      "Epoch 12/30\n",
      "25/25 - 3s - loss: 0.1078 - accuracy: 0.9686 - val_loss: 0.4298 - val_accuracy: 0.8764 - 3s/epoch - 118ms/step\n",
      "Epoch 13/30\n",
      "25/25 - 3s - loss: 0.0867 - accuracy: 0.9824 - val_loss: 0.4023 - val_accuracy: 0.8764 - 3s/epoch - 119ms/step\n",
      "Epoch 14/30\n",
      "25/25 - 3s - loss: 0.0444 - accuracy: 0.9849 - val_loss: 0.4542 - val_accuracy: 0.8652 - 3s/epoch - 126ms/step\n",
      "Epoch 15/30\n",
      "25/25 - 3s - loss: 0.0490 - accuracy: 0.9874 - val_loss: 0.5398 - val_accuracy: 0.8652 - 3s/epoch - 122ms/step\n",
      "Epoch 16/30\n",
      "25/25 - 3s - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.5700 - val_accuracy: 0.8764 - 3s/epoch - 121ms/step\n",
      "Epoch 17/30\n",
      "25/25 - 3s - loss: 0.0292 - accuracy: 0.9937 - val_loss: 0.5702 - val_accuracy: 0.8989 - 3s/epoch - 124ms/step\n",
      "Epoch 18/30\n",
      "25/25 - 3s - loss: 0.0216 - accuracy: 0.9975 - val_loss: 0.6944 - val_accuracy: 0.8989 - 3s/epoch - 124ms/step\n",
      "Epoch 19/30\n",
      "25/25 - 3s - loss: 0.0146 - accuracy: 0.9975 - val_loss: 0.7264 - val_accuracy: 0.8764 - 3s/epoch - 125ms/step\n",
      "Epoch 20/30\n",
      "25/25 - 3s - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.6209 - val_accuracy: 0.8876 - 3s/epoch - 123ms/step\n",
      "Epoch 21/30\n",
      "25/25 - 3s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.8764 - 3s/epoch - 122ms/step\n",
      "Epoch 22/30\n",
      "25/25 - 3s - loss: 0.0748 - accuracy: 0.9837 - val_loss: 0.6457 - val_accuracy: 0.8202 - 3s/epoch - 132ms/step\n",
      "Epoch 23/30\n",
      "25/25 - 3s - loss: 0.0411 - accuracy: 0.9925 - val_loss: 0.4278 - val_accuracy: 0.8989 - 3s/epoch - 129ms/step\n",
      "Epoch 24/30\n",
      "25/25 - 3s - loss: 0.0168 - accuracy: 0.9987 - val_loss: 0.6335 - val_accuracy: 0.8427 - 3s/epoch - 131ms/step\n",
      "Epoch 25/30\n",
      "25/25 - 3s - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.5025 - val_accuracy: 0.8989 - 3s/epoch - 129ms/step\n",
      "Epoch 26/30\n",
      "25/25 - 3s - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.5387 - val_accuracy: 0.8989 - 3s/epoch - 137ms/step\n",
      "Epoch 27/30\n",
      "25/25 - 3s - loss: 0.0086 - accuracy: 0.9987 - val_loss: 0.6308 - val_accuracy: 0.8876 - 3s/epoch - 122ms/step\n",
      "Epoch 28/30\n",
      "25/25 - 3s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.8427 - 3s/epoch - 125ms/step\n",
      "Epoch 29/30\n",
      "25/25 - 3s - loss: 0.0102 - accuracy: 0.9987 - val_loss: 0.5636 - val_accuracy: 0.8876 - 3s/epoch - 125ms/step\n",
      "Epoch 30/30\n",
      "25/25 - 3s - loss: 0.0113 - accuracy: 0.9987 - val_loss: 0.6011 - val_accuracy: 0.8876 - 3s/epoch - 128ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb6a4e8fa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(dict_size, embedding_dim),\n",
    "                             tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "training_labels_seq = np.array(\n",
    "    label_tokenizer.texts_to_sequences(train_categories))\n",
    "tuning_labels_seq = np.array(\n",
    "    label_tokenizer.texts_to_sequences(tuning_categories))\n",
    "\n",
    "model.fit(train_padded, training_labels_seq, epochs=num_epochs,\n",
    "          validation_data=(tuning_padded, tuning_labels_seq), verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f954fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = input(\"Name of unlabeled list of testing file: \")\n",
    "\n",
    "test_list = open(test_file_name).read().splitlines()\n",
    "\n",
    "test_articles = []\n",
    "\n",
    "for line in test_list:\n",
    "    article = open(line).read()\n",
    "    article = article.lower()\n",
    "    article = REPLACE_BY_SPACE_RE.sub(' ', article)\n",
    "    article = BAD_SYMBOLS_RE.sub('', article)\n",
    "    for word in stopwords:\n",
    "        token = \" \" + word + \" \"\n",
    "        article = article.replace(token, ' ')\n",
    "    test_articles.append(article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08948abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_articles)\n",
    "test_padded = pad_sequences(\n",
    "    test_sequences, maxlen=numlen, padding=\"post\", truncating=\"post\")\n",
    "choice = model.predict(test_padded)\n",
    "\n",
    "predictions = []\n",
    "inv_map = {value: key for key, value in label_tokenizer.word_index.items()}\n",
    "\n",
    "for idx in range (0, len(choice)):\n",
    "    index = np.argmax(choice[idx])\n",
    "    predictions.append(inv_map[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e18c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = input(\n",
    "    \"Enter name of output file: \")\n",
    "\n",
    "output_file = open(output_name, 'w')\n",
    "\n",
    "\n",
    "test_file = open(test_file_name).read().splitlines()\n",
    "\n",
    "i = 0\n",
    "for line in test_file:\n",
    "        line = line.strip('\\n')\n",
    "        category = predictions[i]\n",
    "        output_line = line + ' ' + category.capitalize() + '\\n'\n",
    "        output_file.write(output_line)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "output_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73545c549cc23bacda538485df9597c4b6952faae3d99e074994c7c010c63352"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
